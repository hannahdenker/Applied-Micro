<head>
  <link rel="stylesheet" type="text/css" href="stmarkdown.css">
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>
<script type="text/javascript">
document.addEventListener('DOMContentLoaded', function() {
    htmlTableOfContents();
} );                        

function htmlTableOfContents( documentRef ) {
    var documentRef = documentRef || document;
    var toc = documentRef.getElementById("toc");
//  Use headings inside <article> only:
//  var headings = [].slice.call(documentRef.body.querySelectorAll('article h1, article h2, article h3, article h4, article h5, article h6'));
    var headings = [].slice.call(documentRef.body.querySelectorAll('h1, h2, h3, h4, h5, h6'));
    headings.forEach(function (heading, index) {
        var ref = "toc" + index;
        if ( heading.hasAttribute( "id" ) ) 
            ref = heading.getAttribute( "id" );
        else
            heading.setAttribute( "id", ref );

        var link = documentRef.createElement( "a" );
        link.setAttribute( "href", "#"+ ref );
        link.textContent = heading.textContent;

        var div = documentRef.createElement( "div" );
        div.setAttribute( "class", heading.tagName.toLowerCase() );
        div.appendChild( link );
        toc.appendChild( div );
    });
}


try {
    module.exports = htmlTableOfContents;
} catch (e) {
    // module.exports is not defined
}
</script>
</head>
<h1><a href="#problem-set-3" id="problem-set-3">Problem Set 3</a></h1>
<p>Course: ECON 8848<br />
Author: Hannah Denker<br />
Collaborators: Michelle Doughty, Rimjhim Saxana, &amp; Dan Mangan<br />
Date: 16 Feb 2021</p>
<nav id="toc"><strong><font size="5">Table of Contents</font></strong></nav>
<h2><a href="#rq-does-smoking-affect-birthweight" id="rq-does-smoking-affect-birthweight">RQ: Does smoking affect birthweight?</a></h2>
<p>It&rsquo;s helpful to understand that each observation in this dataset represents a newborn.</p>
<h2><a href="#1-drawing-from-various-distributions" id="1-drawing-from-various-distributions">1. Drawing from Various Distributions</a></h2>
<h3><a href="#11-independent-draws" id="11-independent-draws">1.1 Independent Draws</a></h3>
<p>In this section of code, I am going to draw variables for 10,000 observations from three different distributions. To check that they were created correctly, I will also make histograms for each.</p>
<pre><code>.         ***Set obs 
.         drop _all

.         set obs 10000
number of observations (_N) was 0, now 10,000

.         set seed 123456789

. 
.         ***Create var x1 from uniform distribution
.         cap drop x1

.         generate x1 = runiform(4,6)

. 
.         ***Create var x2 from normal distribution
.         cap drop x2

.         generate x2 = rnormal(3,2) // sigma^2=4, so using 2 in the &quot;sd&quot; spot here. 

. 
.         ***Create var x3 from truncated normal distribution
.         ***To generate random var over the interval [a,b), use a+(b-a)*runiform().
.         cap drop x3

.         generate x3= invnorm(0.5+(.5)*runiform(0,3)) // sigma^2=9. 
(6,676 missing values generated)

. 
. 
.         ***Check each with histograms
.         forvalues i=1/3 {
  2.                 qui su x`i', detail
  3.                 local m=r(mean)
  4.                 local sd=r(sd)
  5.                 local low = `m'-`sd'
  6.                 local high=`m'+`sd'
  7.                 local textp= `m'+1
  8. 
.                 hist x`i', ///
&gt;                 xline(`low', lc(blue)) xline(`high', lc(blue)) scale(0.5) ///
&gt;                 xline(`m', lc(black))  ///
&gt;                 name(x`i', replace)
  9.         }
(bin=40, start=4.0003695, width=.04998785)
(bin=40, start=-5.9366045, width=.43979023)
(bin=35, start=.00096494, width=.10705241)

.         
</code></pre>
<h3><a href="#12-joint-distribution" id="12-joint-distribution">1.2 Joint Distribution</a></h3>
<p>We are going to now create a dataset of two variables using a joint distribution (bivariate normal) where:</p>
<ul>
<li> $\mu_1$ = 0, $\mu_2$ = 0  </li> 
<li> $\sigma_1$ = 1, $\sigma_2$ = 2  </li> 
<li> $\rho_1$ = 0.6 </li> 
</ul>
<p>We can check that we drew from the distribution correctly be making a scatter plot showing the two variables we are making plotted against each other. We should see a moderate positive correlation between the two variables ($\rho_1$ = 0.6) with both variables spread around 0, but with a larger spread in x2.</p>
<pre><code>.         
.         ***Create two variables to capture the joint distribution
.         drop _all

.         set obs 1000
number of observations (_N) was 0, now 1,000

.         set seed 123456789

. 
.         matrix mu = (0,0) 

.         matrix sigma = (1,0.6 \ 0.6, 4) // need the variance instead of sigma^2 for matrix

.         drawnorm x1 x2, mean(mu) cov(sigma) 

. 
.         ***Scatter x1 and x2 to check the distribution draws. 
.         scatter x2 x1

. 
</code></pre>
<h2><a href="#2-the-world-series" id="2-the-world-series">2. The World Series</a></h2>
<p>RQ: What is the resulting probability of the better team winning the World Series when an additional game is played? We will use simulation techniques to answer this question.</p>
<h3><a href="#21-the-program" id="21-the-program">2.1 The Program</a></h3>
<p>First, we want to write an rclass program to simulate the binary outcome of a champtionship series with a given number of games. This program will determine the outcome of <em>n</em> games when the &ldquo;best&rdquo; team has a known probability of winning. We are most interested in the primary internal return that indicates whether the better team wins the champtionship. Assume the following:</p>
<ul>
<li> There is a known probability that each team wins with the probability that the "best" team wins 0.5 < p < 1. </li> 
<li> The outcome of each game is independent of the previous games. </li> 
</ul>
<pre><code>. 
. cap program drop p1

.         program define p1, rclass
  1.         version 10.1 // I'm using v15.1, but setting old version here for users w/o update. 
  2.         syntax[, n(integer 100) prob(real 0.51)]
  3.         
.         drop _all
  4.         set obs `n'
  5.         gen draws = rbinomial(1,`prob') // take 01 dummy to indicate win lose
  6.         quietly sum draws
  7.         return scalar wins = (r(mean) &gt;=.5) //prob greater than .5 indicates win
  8. end     

. 
</code></pre>
<h3><a href="#22-simulation" id="22-simulation">2.2 Simulation</a></h3>
<p>Over 1000 iterations, what percent of series are won when the winner is determined by:</p>
<ul>
<li> A "best of 3" game series </li> 
<li> A "best of 5" game series </li> 
<li> A "best of 7" game series </li> 
</ul>
<p>when p=.60</p>
<pre><code>. forvalues p = 0.6(0.2)0.8 {
  2.   forvalues n = 3(2)7 {
  3.         qui simulate wins = r(wins), nodots reps(1000) seed(123456789) : p1, n(`n') prob(`p'
&gt; )
  4.         qui su, detail
  5.         loc sd = round(`r(sd)', .001)
  6.         loc mean = (round(`r(mean)', .0001))*100
  7.         di in red (&quot; &quot;)
  8.         di in red (&quot;With a World Series lasting `n' games &quot;)
  9.         di in red (&quot;and a team's probability of winning equal to `p', &quot;)
 10.         di in red (&quot;they will win the world series `mean'% of the time.&quot;)
 11.         di in red (&quot;SD: `sd'&quot;)
 12.         di in red (&quot; &quot;)
 13.  } // closes forvalues n
 14. } // closes forvalues p         
 
With a World Series lasting 3 games 
and a team's probability of winning equal to .6, 
they will win the world series 63.1% of the time.
SD: .483
 
 
With a World Series lasting 5 games 
and a team's probability of winning equal to .6, 
they will win the world series 67.90000000000001% of the time.
SD: .467
 
 
With a World Series lasting 7 games 
and a team's probability of winning equal to .6, 
they will win the world series 72.40000000000001% of the time.
SD: .447
 
 
With a World Series lasting 3 games 
and a team's probability of winning equal to .8, 
they will win the world series 87.09999999999999% of the time.
SD: .335
 
 
With a World Series lasting 5 games 
and a team's probability of winning equal to .8, 
they will win the world series 93.7% of the time.
SD: .243
 
 
With a World Series lasting 7 games 
and a team's probability of winning equal to .8, 
they will win the world series 96.2% of the time.
SD: .191
 

</code></pre>
<p><strong>What would you say to the two sides in the argument?</strong></p>
<p>The group that thought increases the number of games would increase the probability that the better tream wins is correct. We see from the simulation that in both instances where a team&rsquo;s probability of winning is greater than 50/50, their probability of winning the world series increased each time we added hypothetical games.</p>
<p>Additionally, increasing the number of games from 3 games to 7 results in about a 9% improved chance of winning<br />
the World Series for the better team. One could argue that this is not a large improvement, but I think the argument could be made that this is enough to warrent additional games since it&rsquo;s such an important title to sports fans in addition to being a chance for extra revenue.</p>
<h2><a href="#3-regression-analysis" id="3-regression-analysis">3. Regression analysis</a></h2>
<p>We now are going to investigate the properties of standard OLS regression under ideal conditions. To do this I am going to do the following:</p>
<ul>
<li> Write an rclass program that creates simulated data. </li> 
<li> Run analysis. </li> 
<li> Store results using internal returns. </li> 
<li> Simulate the process 1,000 times. </li> 
</ul>
<h3><a href="#31-the-dgp" id="31-the-dgp">3.1 The DGP</a></h3>
<pre><code>. cap program drop p2

.         program defi p2, rclass
  1.         version 10.1
  2.         syntax[, n(integer 30) beta(real 1)] 
  3.     
.     drop _all 
  4.     set obs `n' 
  5.     cap drop x 
  6.     generate x = runiform(0,10)
  7. 
.     cap drop e 
  8.     generate e = rnormal(0,1)
  9. 
.     cap drop y
 10.     generate y = `beta'*x + e
 11. 
.     reg y x
 12.    
.     return scalar betahat = _b[x] 
 13.         return scalar se = _se[x]
 14. end 

</code></pre>
<h3><a href="#32-monte-carlo" id="32-monte-carlo">3.2 Monte Carlo</a></h3>
<pre><code>.         di in red &quot;...Simulation 1/3 running...&quot;
...Simulation 1/3 running...

.         simulate betahat=r(betahat) se=r(se), nodots reps(5000) seed(123456789) : p2 

      command:  p2
      betahat:  r(betahat)
           se:  r(se)


.         di in red &quot;Simulation finished.&quot;
Simulation finished.

.         qui su betahat, detail

.         loc betahat_mean = `r(mean)'

.         loc betahat_sd = `r(sd)'

. 
.         qui su se, detail

.         loc se_mean = `r(mean)'

. 
</code></pre>
<p><strong>Is the average $\hat{\beta}$ near the true value?</strong></p>
<p>The mean value of $\hat{\beta}$ is        0.998  and the standard deviation of $\hat{\beta}$ is about        0.066 . We know that the true value of $\beta$ = 1, which is less than 0.01 different from our simulated mean.</p>
<p><strong>Is the average standard error close to the true standard deviation of $\hat{\beta}$?</strong></p>
<p>The mean value of the standard error for $\hat{\beta}$ is        0.064. I think the mean of the errors should be 0 if we assume that the errors are random and normally distributed so our estimate looks fairly reasonable compared to that.</p>
<p><strong>If you used a critical value of |1.96|, what percent of the time would you reject this null hypothesis, even though it is true?</strong></p>
<p>We are going to create a variable that represents the t-statistic for each $\hat{\beta}$ and standard error we estimated. To do this, we need to calculate the difference between the $\hat{\beta}$ estimate and the null hypothesis ($\beta$ = 1) and divide this difference by the estimated standard error on the coefficient.</p>
<p>We will then create a variable that is equal to 0 if the t-statistic is <em>less than</em> 1.96 and 1 when the t-statistic is <em>greater than</em> than 1.96, which indicates that the estimated $\beta$ is greater than two standard error units away from the null hypothesis that $\beta$ = 1.</p>
<pre><code>.         cap drop tstat

.         generate tstat = (betahat - 1)/se

. 
.         cap drop rejectnull 

.         generate rejectnull = tstat&gt;=1.96

. 
.         su rejectnull, detail

                         rejectnull
-------------------------------------------------------------
      Percentiles      Smallest
 1%            0              0
 5%            0              0
10%            0              0       Obs               5,000
25%            0              0       Sum of Wgt.       5,000

50%            0                      Mean              .0314
                        Largest       Std. Dev.      .1744137
75%            0              1
90%            0              1       Variance       .0304201
95%            0              1       Skewness        5.37397
99%            1              1       Kurtosis       29.87955

.         loc pct = `r(mean)'*100

. 
</code></pre>
<p>When we summarize the &ldquo;rejectnull&rdquo; variable created comparing the t-statistic to 1.96, we see that it has a mean of 0.031. This would lead us to incorrectly reject the null           3.1% of the time.</p>
<p><strong>Run a simulation with a sample size of 100 with $\beta$ = 1. Compare this distribution with a distribution that has a sample size of 30.</strong></p>
<p>We create another program exactly the same as &ldquo;p2&rdquo;, except we are using a sample of <em>n</em>=100 for each distribution draw. We will run the simulation with this new program using the same number of reps. The mean value of $\hat{\beta}$ is        1.000, and the mean value of the standard error for $\hat{\beta}$ is        0.035.</p>
<p>When we do this, we see that both the estimated $\hat{\beta}$ and its standard error are slightly closer to the true population values ($\beta$=1 and $\sigma$=0).</p>
<h2><a href="#4-heteroskedasticity-and-robust-standard-errors" id="4-heteroskedasticity-and-robust-standard-errors">4. Heteroskedasticity and Robust Standard Errors</a></h2>
<h3><a href="#41-the-dgp" id="41-the-dgp">4.1 The DGP</a></h3>
<pre><code>. cap program drop p4

.         program defi p4, rclass
  1.         version 10.1
  2.         syntax[, n(integer 100) beta(real 1) lambda(real 0)] 
  3.     
.     drop _all 
  4.     set obs `n' 
  5.     cap drop x 
  6.     generate x = runiform(0,10)
  7. 
.     cap drop e 
  8.     generate e = rnormal(0,1)
  9. 
.     cap drop n 
 10.     generate n = rnormal(0,1)
 11. 
.     cap drop y
 12.     generate y = `beta'*x + e + `lambda'*n*x 
 13.     
.     reg y x  
 14.    
.     return scalar betahat = _b[x] 
 15.         return scalar se = _se[x]
 16. 
.         reg y x , robust
 17.         return scalar robust_se = _se[x]
 18. 
. end

. 
</code></pre>
<h3><a href="#42-monte-carlo" id="42-monte-carlo">4.2 Monte Carlo</a></h3>
<pre><code>.         di in red &quot;...Simulation 3/3 running...&quot;
...Simulation 3/3 running...

.         simulate betahat=r(betahat) se=r(se) robust_se=r(robust_se), ///
&gt;                 nodots reps(1000) seed(123456789) : p4, n(250)

      command:  p4, n(250)
      betahat:  r(betahat)
           se:  r(se)
    robust_se:  r(robust_se)


.         di in red &quot;Simulation finished.&quot;
Simulation finished.

. 
.         sum     

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
     betahat |      1,000     .999479    .0219759   .9201213   1.069664
          se |      1,000    .0218886    .0011432   .0180901   .0253875
   robust_se |      1,000    .0218317    .0014794   .0173514   .0269516

</code></pre>
<p><strong>Is there a substantial difference between the two estimates of the standard errors?</strong></p>
<p><strong>Do you get similar rejection rates with a critical value of |1.96|?</strong></p>
<p><strong>Is there a substantial difference between the two estimates of the standard erros?</strong></p>
<p><strong>If so, which one is closer, on average, to the true standard deviation of $\hat{\beta}$?</strong></p>
<p><strong>Do you get similar rejection rates with a critical value of |1.96|?</strong></p>
<p><strong>Will you ever run a regression with classical standard errors again?</strong></p>
