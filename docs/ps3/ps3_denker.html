<head>
  <link rel="stylesheet" type="text/css" href="stmarkdown.css">
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>
<script type="text/javascript">
document.addEventListener('DOMContentLoaded', function() {
    htmlTableOfContents();
} );                        

function htmlTableOfContents( documentRef ) {
    var documentRef = documentRef || document;
    var toc = documentRef.getElementById("toc");
//  Use headings inside <article> only:
//  var headings = [].slice.call(documentRef.body.querySelectorAll('article h1, article h2, article h3, article h4, article h5, article h6'));
    var headings = [].slice.call(documentRef.body.querySelectorAll('h1, h2, h3, h4, h5, h6'));
    headings.forEach(function (heading, index) {
        var ref = "toc" + index;
        if ( heading.hasAttribute( "id" ) ) 
            ref = heading.getAttribute( "id" );
        else
            heading.setAttribute( "id", ref );

        var link = documentRef.createElement( "a" );
        link.setAttribute( "href", "#"+ ref );
        link.textContent = heading.textContent;

        var div = documentRef.createElement( "div" );
        div.setAttribute( "class", heading.tagName.toLowerCase() );
        div.appendChild( link );
        toc.appendChild( div );
    });
}


try {
    module.exports = htmlTableOfContents;
} catch (e) {
    // module.exports is not defined
}
</script>
</head>
<h1><a href="#problem-set-3" id="problem-set-3">Problem Set 3</a></h1>
<p>Course: ECON 8848<br />
Author: Hannah Denker<br />
Collaborators: Michelle Doughty &amp; Dan Mangan<br />
Date: 20 Feb 2021</p>
<nav id="toc"><strong><font size="5">Table of Contents</font></strong></nav>
<h2><a href="#rq-does-smoking-affect-birthweight" id="rq-does-smoking-affect-birthweight">RQ: Does smoking affect birthweight?</a></h2>
<p>It&rsquo;s helpful to understand that each observation in this dataset represents a newborn.</p>
<h2><a href="#1-drawing-from-various-distributions" id="1-drawing-from-various-distributions">1. Drawing from Various Distributions</a></h2>
<h3><a href="#11-independent-draws" id="11-independent-draws">1.1 Independent Draws</a></h3>
<p>In this section of code, I am going to draw variables for 10,000 observations from three different distributions. To check that they were created correctly, I will also make histograms for each.</p>
<pre><code>.         ***Set obs 
.         drop _all

.         set obs 10000
number of observations (_N) was 0, now 10,000

.         set seed 123456789

. 
.         ***Create var x1 from uniform distribution
.         cap drop x1

.         generate x1 = runiform(4,6)

. 
.         ***Create var x2 from normal distribution
.         cap drop x2

.         generate x2 = rnormal(3,2) // sigma^2=4, so using 2 in the &quot;sd&quot; spot here. 

. 
.         ***Create var x3 from truncated normal distribution
.         ***We want positive values only, so pulling using .5 as starting point. 
.         cap drop x3

.         generate x3 = invnorm(runiform(.5,1))*3 // sigma^2=9. 

. 
.         ***Check each with histograms
.         forvalues i=1/3 {
  2.                 qui su x`i', detail
  3.                 local m=r(mean)
  4.                 local sd=r(sd)
  5.                 local low = `m'-`sd'
  6.                 local high=`m'+`sd'
  7.                 local textp= `m'+1
  8. 
.                 hist x`i', ///
&gt;                 xline(`low', lc(blue)) xline(`high', lc(blue)) scale(0.5) ///
&gt;                 xline(`m', lc(black))  ///
&gt;                 name(x`i', replace)
  9.         } // close forvalues loop 
(bin=40, start=4.0003695, width=.04998785)
(bin=40, start=-5.9366045, width=.43979023)
(bin=40, start=.00096494, width=.27422698)

.         
</code></pre>
<h3><a href="#12-joint-distribution" id="12-joint-distribution">1.2 Joint Distribution</a></h3>
<p>We are going to now create a dataset of two variables using a joint distribution (bivariate normal) where:</p>
<ul>
<li> $\mu_1$ = 0, $\mu_2$ = 0  </li> 
<li> $\sigma_1$ = 1, $\sigma_2$ = 2  </li> 
<li> $\rho_1$ = 0.6 </li> 
</ul>
<p>We can check that we drew from the distribution correctly be making a scatter plot showing the two variables we are making plotted against each other. We should see a moderate positive correlation between the two variables ($\rho_1$ = 0.6) with both variables spread around 0, but with a larger spread in x2.</p>
<pre><code>.         
.         ***Create two variables to capture the joint distribution
.         drop _all

.         set obs 1000
number of observations (_N) was 0, now 1,000

.         set seed 123456789

. 
.         matrix mu = (0,0) 

.         matrix sigma = (1,1.2 \ 1.2, 4) // need the variance instead of sigma^2 for matrix

.                                                                         // also using var of x and y and rho to solve for cov.
&gt;  (1.2)
.         drawnorm x1 x2, mean(mu) cov(sigma) 

. 
.         ***Scatter x1 and x2 to check the distribution draws. 
.         scatter x2 x1

. 
</code></pre>
<h2><a href="#2-the-world-series" id="2-the-world-series">2. The World Series</a></h2>
<p>RQ: What is the resulting probability of the better team winning the World Series when an additional game is played? We will use simulation techniques to answer this question.</p>
<h3><a href="#21-the-program" id="21-the-program">2.1 The Program</a></h3>
<p>First, we want to write an rclass program to simulate the binary outcome of a champtionship series with a given number of games. This program will determine the outcome of <em>n</em> games when the &ldquo;best&rdquo; team has a known probability of winning. We are most interested in the primary internal return that indicates whether the better team wins the champtionship. Assume the following:</p>
<ul>
<li> There is a known probability that each team wins with the probability that the "best" team wins 0.5 < p < 1. </li> 
<li> The outcome of each game is independent of the previous games. </li> 
</ul>
<pre><code>. 
. cap program drop p1

.         program define p1, rclass
  1.         version 10.1 // I'm using v15.1, but setting old version here for users w/o update. 
  2.         syntax[, n(integer 7) prob(real 0.6)]   
  3.         drop _all
  4.         set obs `n'
  5.         gen games = rbinomial(1,`prob') // take 01 dummy to indicate win lose
  6.         quietly sum games
  7.         return scalar wins = (r(mean) &gt; .5) //prob greater than .5 indicates win
  8. end     

. 
</code></pre>
<h3><a href="#22-simulation" id="22-simulation">2.2 Simulation</a></h3>
<p>Over 1000 iterations, what percent of series are won when the winner is determined by:</p>
<ul>
<li> A "best of 3" game series </li> 
<li> A "best of 5" game series </li> 
<li> A "best of 7" game series </li> 
</ul>
<p>when p=.60</p>
<pre><code>. forvalues p = 0.6(0.2)0.8 {
  2.   forvalues n = 3(2)7 {
  3.         qui simulate wins = r(wins), nodots reps(1000) seed(123456789) : p1, n(`n') prob(`p')
  4.         qui su, detail
  5.         loc mean : display %3.1f (`r(mean)'*100)
  6.         di in red (&quot; &quot;)
  7.         di in red (&quot;With a World Series lasting `n' games &quot;)
  8.         di in red (&quot;and a team's probability of winning equal to `p', &quot;)
  9.         di in red (&quot;they will win the world series `mean'% of the time.&quot;)
 10.         di in red (&quot; &quot;)
 11.  } // closes forvalues n
 12. } // closes forvalues p         
 
With a World Series lasting 3 games 
and a team's probability of winning equal to .6, 
they will win the world series 63.1% of the time.
 
 
With a World Series lasting 5 games 
and a team's probability of winning equal to .6, 
they will win the world series 67.9% of the time.
 
 
With a World Series lasting 7 games 
and a team's probability of winning equal to .6, 
they will win the world series 72.4% of the time.
 
 
With a World Series lasting 3 games 
and a team's probability of winning equal to .8, 
they will win the world series 87.1% of the time.
 
 
With a World Series lasting 5 games 
and a team's probability of winning equal to .8, 
they will win the world series 93.7% of the time.
 
 
With a World Series lasting 7 games 
and a team's probability of winning equal to .8, 
they will win the world series 96.2% of the time.
 

</code></pre>
<p><strong>What would you say to the two sides in the argument?</strong></p>
<p>The group that thought increases the number of games would increase the probability that the better tream wins is correct. We see from the simulation that in both instances where a team&rsquo;s probability of winning is greater than 50/50, their probability of winning the world series increased each time we added hypothetical games.</p>
<p>Additionally, increasing the number of games from 3 games to 7 results in about a 9% improved chance of winning<br />
the World Series for the better team for both the p=0.60 and the p=0.80 cases. One could argue that this is not a large improvement, but I think the argument could also be made that this is enough to warrent additional games since it&rsquo;s such an important title for teams and fans in addition to being a chance for extra revenue.</p>
<h2><a href="#3-regression-analysis" id="3-regression-analysis">3. Regression analysis</a></h2>
<p>We now are going to investigate the properties of standard OLS regression under ideal conditions. To do this I am going to do the following:</p>
<ul>
<li> Write an rclass program that creates simulated data. </li> 
<li> Run analysis. </li> 
<li> Store results using internal returns. </li> 
<li> Simulate the process 1,000 times. </li> 
</ul>
<h3><a href="#31-the-dgp" id="31-the-dgp">3.1 The DGP</a></h3>
<pre><code>. cap program drop p2

.         program defi p2, rclass
  1.         version 10.1
  2.         syntax[, n(integer 30) beta(real 1)] 
  3.     drop _all 
  4.     set obs `n' 
  5.     cap drop x 
  6.     generate x = runiform(0,10)
  7.     cap drop e 
  8.     generate e = rnormal(0,1)
  9.     cap drop y
 10.     generate y = `beta'*x + e
 11.     reg y x
 12.     return scalar betahat = _b[x] 
 13.         return scalar sehat = _se[x]
 14. end 

</code></pre>
<h3><a href="#32-monte-carlo" id="32-monte-carlo">3.2 Monte Carlo</a></h3>
<pre><code>.         di in red &quot;...Simulation 1/4 running...&quot;
...Simulation 1/4 running...

.         simulate betahat=r(betahat) sehat=r(sehat), nodots reps(1500) seed(9999) : p2 

      command:  p2
      betahat:  r(betahat)
        sehat:  r(sehat)


.         di in red &quot;Simulation finished.&quot;
Simulation finished.

.         qui su betahat, detail

.         loc betahat_mean = `r(mean)'

.         loc betahat_sd = `r(sd)'

. 
.         qui su sehat, detail

.         loc sehat_mean = `r(mean)'

. 
</code></pre>
<p><strong>Is the average $\hat{\beta}$ near the true value?</strong></p>
<p>The mean value of $\hat{\beta}$ is        1.001  and the standard deviation of $\hat{\beta}$ is about        0.064 . We know that the true value of $\beta$ = 1, which is less than 0.01 different from our simulated mean.</p>
<p><strong>Is the average standard error close to the true standard deviation of $\hat{\beta}$?</strong> The mean value of the standard error for $\hat{\beta}$ is        0.064. If we define the &ldquo;true&rdquo; standard deviation of $\hat{\beta}$ to be the standard deviation of our simulated set of $\hat{\beta}$ (       0.064), then we see that the mean value of the standard error we simulated is very close to this value.</p>
<p><strong>If you used a critical value of |1.96|, what percent of the time would you reject this null hypothesis, even though it is true?</strong></p>
<p>We are going to create a variable that represents the t-statistic for each $\hat{\beta}$ and standard error we estimated. To do this, we need to calculate the difference between the $\hat{\beta}$ estimate and the null hypothesis ($\beta$ = 1) and divide this difference by the estimated standard error on the coefficient.</p>
<p>We will then create a variable that is equal to 0 if the t-statistic is <em>less than</em> 1.96 and 1 when the t-statistic is <em>greater than</em> than 1.96, which indicates that the estimated $\beta$ is greater than two standard error units away from the null hypothesis that $\beta$ = 1.</p>
<pre><code>.         cap drop tstat

.         generate tstat = (betahat - 1)/se

. 
.         cap drop rejectnull 

.         generate rejectnull = abs(tstat)&gt;=1.96

. 
.         su rejectnull, detail

                         rejectnull
-------------------------------------------------------------
      Percentiles      Smallest
 1%            0              0
 5%            0              0
10%            0              0       Obs               1,500
25%            0              0       Sum of Wgt.       1,500

50%            0                      Mean           .0526667
                        Largest       Std. Dev.      .2234417
75%            0              1
90%            0              1       Variance       .0499262
95%            1              1       Skewness       4.005363
99%            1              1       Kurtosis       17.04294

.         loc pct = `r(mean)'*100

. 
</code></pre>
<p>When we summarize the &ldquo;rejectnull&rdquo; variable created comparing the t-statistic to 1.96, we see that it has a mean of 0.053. This would lead us to incorrectly reject the null           5.3% of the time. This makes sense given that we associate a p-value of 0.05 with a t-statistic of 1.96.</p>
<p><strong>Run a simulation with a sample size of 100 with $\beta$ = 1. Compare this distribution with a distribution that has a sample size of 30.</strong></p>
<p>We create another program exactly the same as &ldquo;p2&rdquo;, except we are using a sample of <em>n</em>=100 for each distribution draw. We will run the simulation with this new program using the same number of reps. The mean value of the simulated $\hat{\beta}$ is        1.000, and the mean value of the estimated standard errors for $\hat{\beta}$ is        0.035.</p>
<p>When we do this, we see that both the estimated $\hat{\beta}$ and its standard error are slightly closer to the true population values ($\beta$=1 and $\sigma$=       0.035).</p>
<h2><a href="#4-heteroskedasticity-and-robust-standard-errors" id="4-heteroskedasticity-and-robust-standard-errors">4. Heteroskedasticity and Robust Standard Errors</a></h2>
<h3><a href="#41-the-dgp" id="41-the-dgp">4.1 The DGP</a></h3>
<p>We know that OLS assumes that error terms are normally distributed, but that might not be the case. We want to test what happens to our estimated standard errors if errors are heteroskedastic and we don&rsquo;t correct for this in our regression.</p>
<pre><code>. cap program drop p4

.         program defi p4, rclass
  1.         version 10.1
  2.         syntax[, n(integer 250) beta(real 1) lambda(real 0)] 
  3.     
.     drop _all 
  4.     set obs `n' 
  5.     cap drop x 
  6.     generate x = runiform(0,10)
  7. 
.     cap drop e 
  8.     generate e = rnormal(0,1)
  9. 
.     cap drop n 
 10.     generate n = rnormal(0,1)
 11. 
.     cap drop y
 12.     generate y = `beta'*x + e + `lambda'*n*x 
 13.     
.     reg y x  
 14.    
.     return scalar betahat = _b[x] 
 15.         return scalar sehat = _se[x]
 16. 
.         reg y x , robust
 17.         return scalar betahat_rob = _b[x]
 18.         return scalar sehat_rob = _se[x]
 19. end

. 
</code></pre>
<h3><a href="#42a-monte-carlo-homoskedastic" id="42a-monte-carlo-homoskedastic">4.2a Monte Carlo - Homoskedastic</a></h3>
<pre><code>.         di in red &quot;...Simulation 3/4 running...&quot;
...Simulation 3/4 running...

.         simulate betahat=r(betahat) sehat=r(sehat) betahat_rob=r(betahat_rob) sehat_rob=r(sehat_rob), ///
&gt;                 nodots reps(1000) seed(123456789) : p4, n(250)

       command:  p4, n(250)
       betahat:  r(betahat)
         sehat:  r(sehat)
   betahat_rob:  r(betahat_rob)
     sehat_rob:  r(sehat_rob)


.         di in red &quot;Simulation finished.&quot;
Simulation finished.

. 
.         sum 

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
     betahat |      1,000     .999479    .0219759   .9201213   1.069664
       sehat |      1,000    .0218886    .0011432   .0180901   .0253875
 betahat_rob |      1,000     .999479    .0219759   .9201213   1.069664
   sehat_rob |      1,000    .0218317    .0014794   .0173514   .0269516

. 
.         sum betahat

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
     betahat |      1,000     .999479    .0219759   .9201213   1.069664

.         loc beta_mean=`r(mean)'

. 
.         sum sehat

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
       sehat |      1,000    .0218886    .0011432   .0180901   .0253875

.         loc sehat_mean=`r(mean)'

. 
.         sum sehat_rob

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
   sehat_rob |      1,000    .0218317    .0014794   .0173514   .0269516

.         loc sehat_rob_mean=`r(mean)'    

. 
.         cap drop tstat_1

.         generate tstat_1 = (betahat - 1)/sehat

. 
.         cap drop rejectnull_1 

.         generate rejectnull_1 = abs(tstat_1) &gt;= 1.96

. 
.         cap drop tstat_2

.         generate tstat_2 = (betahat - 1)/sehat_rob

. 
.         cap drop rejectnull_2 

.         generate rejectnull_2 = abs(tstat_2) &gt;= 1.96

</code></pre>
<p><strong>Is there a substantial difference between the two estimates of the standard errors?</strong></p>
<p>There&rsquo;s not a difference between the means of the estimated standard errors calculated with/without the robust regression option. The mean value of the standard errors estimated <em>without</em> the robust option is        0.022 while the mean value of the standard errors estimated <em>with</em> the robust option is        0.022. Both are very close to the true standard deviation of simulated $\hat{\beta}$.</p>
<p><strong>Do you get similar rejection rates with a critical value of |1.96|?</strong></p>
<pre><code>.         
.         su rejectnull*, detail

                        rejectnull_1
-------------------------------------------------------------
      Percentiles      Smallest
 1%            0              0
 5%            0              0
10%            0              0       Obs               1,000
25%            0              0       Sum of Wgt.       1,000

50%            0                      Mean               .055
                        Largest       Std. Dev.      .2280943
75%            0              1
90%            0              1       Variance        .052027
95%            1              1       Skewness       3.903847
99%            1              1       Kurtosis       16.24002

                        rejectnull_2
-------------------------------------------------------------
      Percentiles      Smallest
 1%            0              0
 5%            0              0
10%            0              0       Obs               1,000
25%            0              0       Sum of Wgt.       1,000

50%            0                      Mean               .055
                        Largest       Std. Dev.      .2280943
75%            0              1
90%            0              1       Variance        .052027
95%            1              1       Skewness       3.903847
99%            1              1       Kurtosis       16.24002

. 
</code></pre>
<p>Yes, the rejection rates for both types of estimated standard errors is about 5.5%.</p>
<h3><a href="#42b-dgp-monte-carlo-heteroskedastic" id="42b-dgp-monte-carlo-heteroskedastic">4.2b DGP &amp; Monte Carlo - Heteroskedastic</a></h3>
<p>We are going to repeat the data generating process and simulate data when heteroskedasticity is present in the sample. We can then compare differences between estimated standard errors using this process to those estimated in the homoskedastic sample data.</p>
<pre><code>.         di in red &quot;...Simulation 4/4 running...&quot;
...Simulation 4/4 running...

.         simulate betahat=r(betahat) sehat=r(sehat) betahat_rob=r(betahat_rob) sehat_rob=r(sehat_rob), ///
&gt;                 nodots reps(1000) seed(123456789) : p4, n(250) lambda(4)

       command:  p4, n(250) lambda(4)
       betahat:  r(betahat)
         sehat:  r(sehat)
   betahat_rob:  r(betahat_rob)
     sehat_rob:  r(sehat_rob)


.         di in red &quot;Simulation finished.&quot;
Simulation finished.

. 
.         sum betahat

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
     betahat |      1,000    1.005665    .5542005  -.6530894   3.138697

.         loc beta_mean=`r(mean)'

. 
.         sum sehat

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
       sehat |      1,000    .5077953    .0357832   .4080233   .6232379

.         loc sehat_mean=`r(mean)'

. 
.         sum sehat_rob

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
   sehat_rob |      1,000    .5532046    .0542389   .3989905   .7677106

. 
.         loc sehat_rob_mean=`r(mean)'    

. 
.         cap drop tstat_1

.         generate tstat_1 = (betahat - 1)/sehat

. 
.         cap drop rejectnull_1 

.         generate rejectnull_1 = abs(tstat_1) &gt;= 1.96

. 
.         cap drop tstat_2

.         generate tstat_2 = (betahat - 1)/sehat_rob

. 
.         cap drop rejectnull_2 

.         generate rejectnull_2 = abs(tstat_2) &gt;= 1.96

</code></pre>
<p><strong>Is there a substantial difference between the two estimates of the standard errors?</strong></p>
<pre><code>.         su 

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
     betahat |      1,000    1.005665    .5542005  -.6530894   3.138697
       sehat |      1,000    .5077953    .0357832   .4080233   .6232379
 betahat_rob |      1,000    1.005665    .5542005  -.6530894   3.138697
   sehat_rob |      1,000    .5532046    .0542389   .3989905   .7677106
     tstat_1 |      1,000    .0090047    1.095881  -3.295508   3.667933
-------------+---------------------------------------------------------
rejectnull_1 |      1,000        .078    .2683058          0          1
     tstat_2 |      1,000    .0086923    1.007248  -2.916028   3.363433
rejectnull_2 |      1,000        .051    .2201078          0          1

</code></pre>
<p>When more heteroskedasticity is present in the data, we see substantial differences in the estimated standard errors. The robust standard error is        0.553 while the regular standard error is        0.508, which is about a 9% difference in estimated values.</p>
<p><strong>If so, which one is closer, on average, to the true standard deviation of $\hat{\beta}$?</strong> We also see that the robust standard error remains close to the true standard deviation of the sample $\hat{\beta}$, and the regular standard error underestimates this &ldquo;true&rdquo; value. These differences may lead us to conclude that differences statistically significant when they are, in fact, a product of heteroskedasticity.</p>
<p><strong>Do you get similar rejection rates with a critical value of |1.96|?</strong></p>
<pre><code>.         su rejectnull*

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
rejectnull_1 |      1,000        .078    .2683058          0          1
rejectnull_2 |      1,000        .051    .2201078          0          1

</code></pre>
<p>The rejection rates are substantively different between the two types of standard error estimation. Using the robust standard errors, we would reject the null hypothesis incorrectly about 5.1% of the time - what we would expect for a t-statistic of 1.96 and the corresponding p-value of 0.05. Using the standard errors estimated without controlling for the heteroskedastic nature of the sample, we would reject the null hypothesis incorrectly about 7.8% of the time. We see that this rejection rate is higher than 5%, meaning that we would reject the null when we shouldn&rsquo;t more often than the rate we assume. In other words, if we don&rsquo;t account for heteroskedasticity in the estimates of our standard error we may overestimate the precision of our coefficients and more freqently conclude that results are statistically significant (not due to chance) when in fact this is not the case.</p>
<p><strong>Will you ever run a regression with classical standard errors again?</strong></p>
<p>It seems pretty easy to just throw on the &ldquo;robust&rdquo; option to our regression specfication. Adding this option seems to help us get estimates we can trust without any obvious downsides.</p>
